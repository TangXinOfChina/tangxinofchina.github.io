<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ChatGPT中RLHF技术介绍</title>
      <link href="/2023/04/17/chatgpt-zhong-rlhf-ji-zhu-jie-shao/"/>
      <url>/2023/04/17/chatgpt-zhong-rlhf-ji-zhu-jie-shao/</url>
      
        <content type="html"><![CDATA[<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>随着OpenAI发布的ChatGPT大火，越来越多人开始关注其中用到的<strong>RLHF</strong>(Reinforcement Learning from Human Feedback)技术,其字面意思是基于人类反馈的强化学习，核心思想是获取生成文本的人工反馈，使用强化学习对语言模型进行优化，旨在于生成人类偏好的答案，增加了答案的合理性和安全性——<strong>使生成模型与人类偏好对齐，提升监督学习的性能天花板</strong>。</p><h3 id="技术原理"><a href="#技术原理" class="headerlink" title="技术原理"></a>技术原理</h3><p>RLHF可以简单分解为三个核心步骤：</p><ul><li>1.预训练语言模型（LM）</li><li>2.收集数据并训练奖励模型</li><li>3.通过强化学习微调 LM</li></ul><p><img src="/images/pasted-2.png" alt="RLHF整体流程" title="RLHF整体流程"></p><h4 id="Step1-预训练语言模型"><a href="#Step1-预训练语言模型" class="headerlink" title="Step1. 预训练语言模型"></a>Step1. 预训练语言模型</h4><p><img src="/images/pasted-1.png" alt="step1. 预训练模型训练过程" title="step1. 预训练模型训练过程"><br>首先，采用经典的预训练目标训练一个语言模型, 也即冷启动阶段的监督策略模型。尽管GTP-3.5已经非常强大，但是它很难理解人类不同类型指令中蕴含的不同意图，也很难判断生成内容是否是高质量的结果。为了让GPT 3.5初步具备理解指令中蕴含的意图，首先会从测试用户提交的prompt(就是指令或问题)中随机抽取一批，靠专业的标注人员，给出指定prompt的高质量答案，然后用这些人工标注好的&lt;prompt,answer&gt;数据来 Fine-tune GPT 3.5模型。经过这个过程，我们可以认为GPT-3.5初步具备了理解人类prompt中所包含意图，并根据这个意图给出相对高质量回答的能力，但仅仅这样做是仍然不够。</p><p>这里可以用额外的文本或者条件对这个 LM 进行微调，例如 OpenAI 对 “更可取” (preferable) 的人工生成文本进行了微调，但并不是 RLHF 必须的一步（虚线表示）</p><h4 id="Step2-训练奖励模型"><a href="#Step2-训练奖励模型" class="headerlink" title="Step2 训练奖励模型"></a>Step2 训练奖励模型</h4><p>这个阶段的主要目的是通过人工标注训练数据，来训练奖励模型模型。</p><p>首先，随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段Fine-tune好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了&lt;prompt, answer1&gt;, &lt;prompt, answer2&gt;….&lt;prompt, answerK&gt; 数据。之后，标注人员对K个结果按照很多标准（上面提到的相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序，给出K个结果的排名顺序。</p><p>接下来，我们准备利用这个排序结果数据来训练回报模型，采取的训练模式其实就是平常经常用到的pair-wise learning to rank。对于K个排序结果，两两组合，形成$C_{K}^2$个训练数据对，ChatGPT采取pair-wise loss来训练Reward Model。</p><p>例如，用图1中例子（D &gt; C &gt; B &gt; A）来讲，loss 应该等于：<br>$loss = - (r(D) - r(C) + r(D) - r(B) + r(D) - r(A) + r(C) - r(B) + … + r(B) - r(A))$</p><p>为了更好的归一化差值，我们对每两项差值都过一个 sigmoid 函数将值拉到 0 ~ 1 之间。可以看到，loss 的值等于排序列表中所有「排在前面项的reward」减去「排在后面项的reward」的和。 而我们希望模型能够「最大化」这个「好句子得分」和「坏句子得分」差值，而梯度下降是做的「最小化」操作，因此加上一个负号。具体的loss function如下所示：</p><p><img src="/images/loss_rm.png" alt="奖励模型loss function" title="奖励模型loss function"><br>综上所述，在这个阶段，首先由冷启动后的监督策略模型为每个prompt产生K个结果，人工根据结果质量由高到低排序，以此作为训练数据，通过pair-wise learning to rank模式来训练回报模型。对于学好的RM模型来说，输入&lt;prompt,answer&gt;，输出结果的质量得分，得分越高说明产生的回答质量越高。</p><p><img src="/images/pasted-3.png" alt="step 2. 奖励模型训练过程" title="step 2. 奖励模型训练过程"></p><h4 id="Step3-通过强化学习微调-LM"><a href="#Step3-通过强化学习微调-LM" class="headerlink" title="Step3 通过强化学习微调 LM"></a>Step3 通过强化学习微调 LM</h4><p>我们将初始语言模型的微调任务建模为强化学习（RL）问题，因此需要定义策略（policy）、动作空间（action space）和奖励函数（reward function）等基本要素：</p><ul><li>策略（policy）：语言模型</li><li>动作空间（action space）：所有token在所有输出位置的排列组合</li><li>观察空间则是可能的输入token序列（即prompt）</li><li>奖励函数：Step2中得到的reward模型 + 约束项</li></ul><p>显然，策略就是基于该语言模型，接收prompt作为输入，然后输出一系列文本（或文本的概率分布）；而动作空间就是词表所有token在所有输出位置的排列组合（单个位置通常有50k左右的token候选）；观察空间则是可能的输入token序列（即prompt），显然也相当大，为词表所有token在所有输入位置的排列组合；而奖励函数（reward）则是基于上一章节我们训好的RM模型计算得到初始reward，再叠加上一个约束项。</p><p>具体来说，给定数据集的提示 x ，将生成两个文本 $y_1$ 和 $y_2$ ，一个来自初始语言模型LM，另一个来自微调策略的当前迭代LM。将来自当前策略的文本传递给 偏好模型RM 得到一个标量的奖励 ${\boldsymbol{r}}_{\theta}$ 。</p><p>为了确保模型输出合理连贯的文本，防止模型在优化中生成胡言乱语文本来愚弄奖励模型提供高奖励值，因此加上一个约束项，使得优化后的语言模型生成文本与初始语言模型不要偏差太多，一般分布差异的惩罚项被设计为输出词分布序列之间的 Kullback–Leibler (KL) 散度的缩放 ${\boldsymbol{r}}<em>{KL}$ ，最终用于惩罚 RL 策略在每个训练批次中生成大幅偏离初始模型，因此RL更新规则的最终奖励是： $r=r</em>{\theta} - {\lambda}{\cdot} {r_{KL}}$ 。</p><p><img src="/images/pasted-4.png" alt="Step3. 强化学习优化语言模型过程" title="Step3. 强化学习优化语言模型过程"></p><h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3><p><img src="/images/pasted-0.png" alt="数据集规模介绍" title="数据集规模介绍"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="%22https://arxiv.org/pdf/2203.02155.pdf%22">论文链接</a></li><li><a href="https://huggingface.co/blog/rlhf">huggingface官方博客</a></li><li><a href="https://zhuanlan.zhihu.com/p/589533490">张俊林:ChatGPT会取代搜索引擎吗</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> PLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bert模型简介</title>
      <link href="/2023/03/19/230319-bertmodeloutput01/"/>
      <url>/2023/03/19/230319-bertmodeloutput01/</url>
      
        <content type="html"><![CDATA[<h4 id="1-简单介绍"><a href="#1-简单介绍" class="headerlink" title="1.简单介绍"></a>1.简单介绍</h4><p><a href="https://www.semanticscholar.org/reader/df2b0e26d0599ce3e70df8a9da02e51594e0e992"><strong>BERT</strong> </a>(Bidirectional Encoder Representation Transformers) 是在2018年由Google AI 团队发布的，网络结构主要采用了Transformer编码器的架构，属于自编码模型。具体而言，BERT是一个多层Transformer的Encoder，输入的Embedding通过一层层的Encoder进行编码转换，再连接到不同的下游任务。</p><!-- <div align=center><img src="./imgs/bert_framework.jpg"><center><font size="2">图1 Bert图示</font></center></div> --><div align="center"><img src="/images/bert_framework.jpg"><center><font size="2">图1 Bert图示</font></center></div><h4 id="2-算法原理"><a href="#2-算法原理" class="headerlink" title="2. 算法原理"></a>2. 算法原理</h4><h4 id="2-1-模型输入"><a href="#2-1-模型输入" class="headerlink" title="2.1 模型输入"></a>2.1 模型输入</h4><p>总体来说，Bert网络结构与Transformer Encoder结构基本相同，<br>其输入向量由三部分组成，分别是<strong>token embedding</strong>, <strong>segment embedding</strong>和<strong>position Embedding</strong>。在分词接入token embedding之前，先进行tokenization处理。这里采用WordPiece嵌入方式，将每个单词进一步拆分成更通用的sub-word，可以避免词汇表过大及减缓OOV的问题。这里<strong>position Embedding</strong>与transformer实现不同，是通过网络学习得到的。</p><!-- <div align=center><img src="./imgs/input_embedings.jpg"><center><font size="2">图2 Bert输入embedding向量图示</font></center></div> --><div align="center"><img src="/images/input_embedings.jpg"><center><font size="2">图2 Bert输入embedding向量图示</font></center></div><h4 id="2-2-预训练任务"><a href="#2-2-预训练任务" class="headerlink" title="2.2 预训练任务"></a>2.2 预训练任务</h4><p>BERT采用两个无监督任务进行参数预训练：</p><ul><li><strong>MLM(Masked Language Model)：</strong> 对句子中的word随机mask，让模型去预测mask掉的单词。在BERT的预训练中采取的策略是随机抽取句子中15%的单词进行处理，其中这15%的单词又按照80%-10%-10%的比例采取不同的处理方式：80%的单词用[MASK]来替换，表示需要预测的部分，10%用随机的词来替换，10%维持原单词不变。这种训练流程可以让模型学习到单词在上下文中的分布表示；</li><li><strong>NSP(Next Sentence Prediction)：</strong> 对于句子对A和B，选取B的时候50%的概率是真实的A的下一句(标签IsNext)，50%的概率是从语料中随机选取的句子(标签NotNext)。通过预训练NSP任务，让模型理解到两个句子之间的关系，从而能够应用于QA和NLI等下游任务中。</li></ul><h4 id="2-3-预训练数据和参数"><a href="#2-3-预训练数据和参数" class="headerlink" title="2.3 预训练数据和参数"></a>2.3 预训练数据和参数</h4><p>以下分项列出BERT预训练过程使用的所有数据和调试参数：</p><ul><li>数据集：BooksCorpus (单词量 800M)，English Wikipedia (单词量 2,500M)</li><li>主要超参：batch_size=256, epochs=40, max_tokens=512, dropout=0.1</li><li>优化器参数：优化器Adam, lr=1e-4, β1=0.9, β2=0.999, L2 weight decay=0.01, lr_warmup=10,000</li><li>激活函数：gelu</li><li>训练损失：mean MLM likelihood + mean NSP likelihood<br>机器配置：BERT(base)使用4个云TPUs，BERT(large)使用16个云TPUs</li><li>训练时长：4天</li><li>加速方式：90%的步数按照128的文本长度训练，剩余10%的步数按照512的文本长度训练</li></ul><h4 id="2-4-BERT的下游任务"><a href="#2-4-BERT的下游任务" class="headerlink" title="2.4 BERT的下游任务"></a>2.4 BERT的下游任务</h4><p>特定任务的模型是通过在BERT基础上增加一个额外的输出层，BERT典型的四个下游任务分别是： </p><ul><li>句子对分类任务：如MNLI, QQP, QNLI, STS-B, MRPC, RTE, SWAG任务； </li><li>单句子分类任务：如SST-2，CoLA任务； </li><li>问答系统任务：如SQuAD v1.1任务； </li><li>单句子标注任务：如CoNLL-2003 NER任务；</li></ul><!-- ![Bert下游任务](./imgs/bert_downstream_task.jpg "Bert下游任务") --><p><img src="/images/bert_downstream_task.jpg" alt="Bert下游任务" title="Bert下游任务"></p><h4 id="3-加载模型"><a href="#3-加载模型" class="headerlink" title="3. 加载模型"></a>3. 加载模型</h4><p>由于从头训练大模型需要耗费大量数据和算力，我们可以从<a href="https://huggingface.co/">HuggingFace</a>中下载其他公司已经训练好的Bert模型来使用。<br>这里简单介绍torch及transformers库加载bert模型，代码如下：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModelbert <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"pre_model/bert-base-chinese"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>bert<span class="token punctuation">)</span></code></pre><!-- ![Bert加载结果](./imgs/load_model_res.png "Bert加载结果") --><p><img src="/images/load_model_res.png" alt="Bert加载结果" title="Bert加载结果"></p><p>模型输入，查看<a href="https://github.com/huggingface/transformers/blob/v4.27.1/src/transformers/models/bert/modeling_bert.py">源码</a>可以看到BertModel的forward函数</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        input_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        token_type_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        position_ids<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        head_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        inputs_embeds<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        encoder_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        encoder_attention_mask<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        past_key_values<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        use_cache<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        output_attentions<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        output_hidden_states<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>        return_dict<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span><span class="token builtin">bool</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Union<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> BaseModelOutputWithPoolingAndCrossAttentions<span class="token punctuation">]</span><span class="token punctuation">:</span></code></pre><p>关键参数介绍：</p><ul><li>input_ids (torch.LongTensor of shape (batch_size, sequence_length)) — 词汇表中输入序列标记的索引</li><li>attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — 对输入数据进行mask，解决pad问题。在 [0, 1] 中选择的掩码值:1 表示未屏蔽的标记，0 表示已屏蔽的标记</li><li>token_type_ids (torch.LongTensor of shape (batch_size, sequence_length), optional) — 分段标记索引以指示输入的第一和第二部分。在 [0, 1] 中选择索引:0对应一个句子A的token，1对应一个句子B的token。</li></ul><p>模型输入我们可以自己构建，也可以利用Transformers中的分词器对象构建，操作如下：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel<span class="token punctuation">,</span> BertTokenizertokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"pre_model/bert-base-chinese"</span><span class="token punctuation">)</span>test_sentence <span class="token operator">=</span> <span class="token string">"我在测试bert"</span>tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode_plus<span class="token punctuation">(</span>text<span class="token operator">=</span>test_sentence<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>convert_ids_to_tokens<span class="token punctuation">(</span>tokens<span class="token punctuation">[</span><span class="token string">'input_ids'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>输出结果如下：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token punctuation">{</span><span class="token string">'input_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">101</span><span class="token punctuation">,</span> <span class="token number">2769</span><span class="token punctuation">,</span> <span class="token number">1762</span><span class="token punctuation">,</span> <span class="token number">3844</span><span class="token punctuation">,</span> <span class="token number">6407</span><span class="token punctuation">,</span> <span class="token number">8815</span><span class="token punctuation">,</span> <span class="token number">8716</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'token_type_ids'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'attention_mask'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">[</span><span class="token string">'[CLS]'</span><span class="token punctuation">,</span> <span class="token string">'我'</span><span class="token punctuation">,</span> <span class="token string">'在'</span><span class="token punctuation">,</span> <span class="token string">'测'</span><span class="token punctuation">,</span> <span class="token string">'试'</span><span class="token punctuation">,</span> <span class="token string">'be'</span><span class="token punctuation">,</span> <span class="token string">'##rt'</span><span class="token punctuation">,</span> <span class="token string">'[SEP]'</span><span class="token punctuation">]</span></code></pre><p>可以看到该方法就已经对输入的句子进行了分词，需要提醒的是：这里的tokenizer分词不同以往的分词方法，使用subword算法（词”bert”就被分开了），并且为语句的句首和句首分别添加了[CLS]、[SEG]符号。  </p><p>模型使用案例：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertModel<span class="token punctuation">,</span> BertTokenizerbert <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"pre_model/bert-base-chinese"</span><span class="token punctuation">)</span>tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"pre_model/bert-base-chinese"</span><span class="token punctuation">)</span>test_sentence <span class="token operator">=</span> <span class="token string">"我在测试bert"</span><span class="token comment"># 指定返回的数据是pytorch中的tensor数据类型</span>tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode_plus<span class="token punctuation">(</span>text<span class="token operator">=</span>test_sentence<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">)</span>model_out <span class="token operator">=</span> bert<span class="token punctuation">(</span><span class="token operator">**</span>tokens<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>model_out<span class="token punctuation">)</span>  <span class="token comment"># model_out是BaseModelOutputWithPoolingAndCrossAttentions对象</span></code></pre><p>模型默认的输出BaseModelOutputWithPoolingAndCrossAttentions对象，可以看到源码中forward函数的返回值输出：</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token keyword">return</span> BaseModelOutputWithPoolingAndCrossAttentions<span class="token punctuation">(</span>            last_hidden_state<span class="token operator">=</span>sequence_output<span class="token punctuation">,</span>            pooler_output<span class="token operator">=</span>pooled_output<span class="token punctuation">,</span>            past_key_values<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>past_key_values<span class="token punctuation">,</span>            hidden_states<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>hidden_states<span class="token punctuation">,</span>            attentions<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>attentions<span class="token punctuation">,</span>            cross_attentions<span class="token operator">=</span>encoder_outputs<span class="token punctuation">.</span>cross_attentions<span class="token punctuation">,</span>        <span class="token punctuation">)</span><span class="token comment"># BaseModelOutputWithPoolingAndCrossAttentions中各参数类型</span><span class="token punctuation">(</span>last_hidden_state<span class="token punctuation">:</span> FloatTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>pooler_output<span class="token punctuation">:</span> FloatTensor <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>hidden_states<span class="token punctuation">:</span> typing<span class="token punctuation">.</span>Optional<span class="token punctuation">[</span>typing<span class="token punctuation">.</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>past_key_values<span class="token punctuation">:</span> typing<span class="token punctuation">.</span>Optional<span class="token punctuation">[</span>typing<span class="token punctuation">.</span>Tuple<span class="token punctuation">[</span>typing<span class="token punctuation">.</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>attentions<span class="token punctuation">:</span> typing<span class="token punctuation">.</span>Optional<span class="token punctuation">[</span>typing<span class="token punctuation">.</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>cross_attentions<span class="token punctuation">:</span> typing<span class="token punctuation">.</span>Optional<span class="token punctuation">[</span>typing<span class="token punctuation">.</span>Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span></code></pre><p>我们经常使用的则是：</p><ul><li><p>last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) — 模型最后一层输出的隐藏状态序列.</p></li><li><p>pooler_output (torch.FloatTensor of shape (batch_size, hidden_size))— 是模型last_hidden_state选取[CLS]位置对应的向量经过一个线性层+tanh()激活函数之后得到的</p></li><li><p>hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True) — 形状为（batch_size, sequence_length, hidden_​​size）的torch.FloatTensor 构成的元组（embedding层 + 每一个transformer encoder的输出），例如bert一般12层，因此该tuple长度为13.</p></li><li><p>hidden_states[-1]就是last_hidden_state</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bert </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JupyterNotebook一些有用操作</title>
      <link href="/2023/03/18/230318-jupyter-trick/"/>
      <url>/2023/03/18/230318-jupyter-trick/</url>
      
        <content type="html"><![CDATA[<h2 id="Jupyter-NoteBook一些有用操作"><a href="#Jupyter-NoteBook一些有用操作" class="headerlink" title="Jupyter NoteBook一些有用操作"></a>Jupyter NoteBook一些有用操作</h2><h3 id="一、配置远程访问JupyterNoteBook"><a href="#一、配置远程访问JupyterNoteBook" class="headerlink" title="一、配置远程访问JupyterNoteBook"></a>一、配置远程访问JupyterNoteBook</h3><p>安装完<a href="https://www.anaconda.com/">Anaconda</a>之后，运行如下shell命令：</p><pre class="language-bash" data-language="bash"><code class="language-bash"><span class="token function">nohup</span> jupyter-notebook <span class="token parameter variable">--port</span><span class="token operator">=</span><span class="token operator">&lt;</span>端口号<span class="token operator">&gt;</span> <span class="token parameter variable">--ip</span><span class="token operator">=</span><span class="token operator">&lt;</span>ip地址<span class="token operator">&gt;</span> --allow-root <span class="token operator">&amp;</span></code></pre><p>运行完成后，复制生成的nohup.txt中的token进行访问。</p><h3 id="二、魔法命令"><a href="#二、魔法命令" class="headerlink" title="二、魔法命令"></a>二、魔法命令</h3><ol><li><p>%magic: 查看所有魔术命令及详细文档；</p></li><li><p>%timeit: 监测代码运行时间；</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">%</span>timeit <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">]</span> </code></pre></li><li><p>%%file:  将脚本代码写入本地Py文件；</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">%</span><span class="token operator">%</span><span class="token builtin">file</span> <span class="token operator">/</span>home<span class="token operator">/</span>ubuntu<span class="token operator">/</span>xxx<span class="token punctuation">.</span>py <span class="token keyword">def</span> <span class="token function">print_func</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hello world"</span><span class="token punctuation">)</span></code></pre></li><li><p>%run:    执行py文件；</p><pre class="language-python" data-language="python"><code class="language-python"><span class="token operator">%</span>run hello<span class="token punctuation">.</span>py</code></pre></li><li><p>!+shell命令：运行shell命令；</p><pre class="language-python" data-language="python"><code class="language-python">!ps <span class="token operator">-</span>aux</code></pre></li><li><p>%debug:  快速debug；</p></li><li><p>%who: 显示当前JupyterNotebook环境中拥有的所有可用变量</p></li><li><p>%prun:   %prun用于计算函数或程序执行每个函数需要多长时间,显示结果更加细节；</p></li><li><p>%history 或者 %hist: 显示之前进行了哪些操作，有时单元格跳转太多忘了之前做了哪些操作；</p></li><li><p>%matplotlib inline：不需要显示地调用pyplot.show()即可自动显示图片；</p></li><li><p>%matplotlib notebook：提供交互式的绘图功能。这个可能会稍微慢一些，因为需要更多的事件进行图片渲染(rendering)。</p></li><li><p>?+变量或者函数：返回关于该函数或者该变量的相关信息</p><pre class="language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>?a?np<span class="token punctuation">.</span>ones</code></pre></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JupyterNotebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/02/26/hello-world/"/>
      <url>/2023/02/26/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="language-bash" data-language="bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
